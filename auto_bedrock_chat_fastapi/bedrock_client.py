"""Bedrock client for AI model interaction"""

import boto3
import json
import asyncio
import logging
import traceback
from typing import Dict, List, Any, Optional, Union
from botocore.exceptions import BotoCoreError, ClientError
from datetime import datetime
import time

from .config import ChatConfig
from .exceptions import BedrockClientError


logger = logging.getLogger(__name__)


class BedrockClient:
    """Amazon Bedrock client for AI model interactions"""
    
    def __init__(self, config: ChatConfig):
        self.config = config
        self._client = None
        self._session = None
        self._last_request_time = 0
        self._request_count = 0
        
        # Initialize AWS session and client
        self._initialize_client()
    
    def _initialize_client(self):
        """Initialize boto3 client for Bedrock"""
        try:
            # Create AWS session
            self._session = boto3.Session(**self.config.get_aws_config())
            
            # Import botocore config for timeout settings
            from botocore.config import Config
            
            # Create client config with increased timeout for large models
            client_config = Config(
                read_timeout=max(120, self.config.timeout),  # At least 2 minutes
                connect_timeout=30,  # Increased connection timeout
                retries={'max_attempts': 3}
            )
            
            # Create Bedrock client
            self._client = self._session.client(
                'bedrock-runtime',
                region_name=self.config.aws_region,
                config=client_config
            )
            
            logger.info(f"Bedrock client initialized for region: {self.config.aws_region}")
            
        except Exception as e:
            raise BedrockClientError(f"Failed to initialize Bedrock client: {str(e)}")
    
    async def chat_completion(
        self,
        messages: List[Dict[str, Any]],
        model_id: Optional[str] = None,
        tools_desc: Optional[Dict] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Main chat completion function called by the plugin
        
        Args:
            messages: List of conversation messages (system prompt should be first message if needed)
            model_id: Bedrock model ID to use
            tools_desc: Tools/functions available to the model
            temperature: Sampling temperature
            max_tokens: Maximum tokens in response
            **kwargs: Additional model parameters
            
        Returns:
            Dict containing the model response, tool calls, and metadata
        """
        
        # Use config defaults if not provided
        model_id = model_id or self.config.model_id
        tools_desc = tools_desc or self.config.tools_desc
        temperature = temperature if temperature is not None else self.config.temperature
        max_tokens = max_tokens or self.config.max_tokens
        
        # Log request if enabled
        if self.config.log_api_calls:
            logger.info(f"Bedrock request: model={model_id}, messages={len(messages)}")
        
        try:
            # Rate limiting
            await self._handle_rate_limiting()
            
            # Manage conversation history to prevent context length issues
            original_count = len(messages)
            messages = self._manage_conversation_history(messages)
            if len(messages) < original_count:
                logger.info(f"Conversation history trimmed from {original_count} to {len(messages)} messages")
            
            # Check and chunk large messages to prevent individual message size issues
            original_message_count = len(messages)
            messages = self._check_and_chunk_messages(messages)
            if len(messages) > original_message_count:
                logger.info(f"Large messages chunked: {original_message_count} -> {len(messages)} messages")
            
            # Prepare the request based on model family
            # logger.debug(f"Sending messages to model {model_id}: {messages}")
            request_body = self._prepare_request_body(
                messages=messages,
                model_id=model_id,
                tools_desc=tools_desc,
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            )
            
            # Make the API call with retries
            response = await self._make_request_with_retries(model_id, request_body)
            # logger.debug(f"Bedrock response: {response}")
            
            # Parse and format the response
            formatted_response = self._parse_response(response, model_id)
            
            # Process any tool calls
            if formatted_response.get("tool_calls"):
                tool_results = await self._execute_tool_calls(
                    formatted_response["tool_calls"],
                    tools_desc
                )
                formatted_response["tool_results"] = tool_results
            
            return formatted_response
            
        except Exception as e:
            logger.exception(f"Chat completion error: {str(e)}")
            
            # Try fallback model if configured
            if self.config.fallback_model and model_id != self.config.fallback_model:
                logger.info(f"Attempting fallback to model: {self.config.fallback_model}")
                try:
                    return await self.chat_completion(
                        messages=messages,
                        model_id=self.config.fallback_model,
                        tools_desc=tools_desc,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        **kwargs
                    )
                except Exception as fallback_error:
                    logger.exception(f"Fallback model also failed: {str(fallback_error)}")
            
            # Handle graceful degradation
            if self.config.graceful_degradation:
                return self._create_error_response(str(e))
            
            raise BedrockClientError(f"Chat completion failed: {str(e)}")
    
    def _prepare_request_body(
        self,
        messages: List[Dict[str, Any]],
        model_id: str,
        tools_desc: Optional[Dict],
        temperature: float,
        max_tokens: int,
        **kwargs
    ) -> Dict[str, Any]:
        """Prepare request body based on model family"""
        
        if model_id.startswith("anthropic.claude") or model_id.startswith("us.anthropic.claude"):
            return self._prepare_claude_request(
                messages, tools_desc, temperature, max_tokens, **kwargs
            )
        elif model_id.startswith("amazon.titan"):
            return self._prepare_titan_request(
                messages, tools_desc, temperature, max_tokens, **kwargs
            )
        elif model_id.startswith("meta.llama"):
            return self._prepare_llama_request(
                messages, tools_desc, temperature, max_tokens, **kwargs
            )
        elif model_id.startswith("openai.gpt-oss"):
            return self._prepare_openai_gpt_request(
                messages, tools_desc, temperature, max_tokens, **kwargs
            )
        else:
            # Generic format
            return self._prepare_generic_request(
                messages, tools_desc, temperature, max_tokens, **kwargs
            )
    
    def _prepare_claude_request(
        self, messages, tools_desc, temperature, max_tokens, **kwargs
    ) -> Dict[str, Any]:
        """Prepare request for Claude models"""
        
        # Extract system prompt from messages if present
        system_prompt = None
        conversation_messages = []
        
        for msg in messages:
            if msg.get("role") == "system":
                system_prompt = msg.get("content", "")
            else:
                conversation_messages.append(msg)
        
        # Use default system prompt if none provided
        if not system_prompt:
            system_prompt = self.config.get_system_prompt()
        
        request_body = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": max_tokens,
            "temperature": temperature,
            "messages": conversation_messages,
            "system": system_prompt
        }
        
        # Note: For newer Claude models, we only use temperature, not top_p
        # to avoid "temperature and top_p cannot both be specified" error
        # If you need top_p instead of temperature, modify the config accordingly
        
        # Add tools if available
        if tools_desc and tools_desc.get("functions"):
            request_body["tools"] = [
                {
                    "name": func["name"],
                    "description": func["description"],
                    "input_schema": func["parameters"]
                }
                for func in tools_desc["functions"]
            ]
        
        return request_body
    
    def _prepare_openai_gpt_request(
        self, messages, tools_desc, temperature, max_tokens, **kwargs
    ) -> Dict[str, Any]:
        """Prepare request for OpenAI GPT OSS models"""
        
        # For OpenAI format, messages can already include system message
        # If no system message is present, add the default one
        has_system_message = any(msg.get("role") == "system" for msg in messages)
        
        formatted_messages = []
        
        if not has_system_message:
            # Add default system message if none present
            formatted_messages.append({
                "role": "system",
                "content": self.config.get_system_prompt()
            })
        
        # Add all conversation messages
        formatted_messages.extend(messages)
        
        request_body = {
            "messages": formatted_messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": kwargs.get("top_p", self.config.top_p),
        }
        
        # Add tools if available
        if tools_desc and tools_desc.get("functions"):
            request_body["tools"] = [
                {
                    "type": "function",
                    "function": {
                        "name": func["name"],
                        "description": func["description"],
                        "parameters": func["parameters"]
                    }
                }
                for func in tools_desc["functions"]
            ]
        
        return request_body
    
    def _prepare_titan_request(
        self, messages, tools_desc, temperature, max_tokens, **kwargs
    ) -> Dict[str, Any]:
        """Prepare request for Titan models"""
        
        # Extract system prompt from messages or use default
        system_prompt = None
        conversation_messages = []
        
        for msg in messages:
            if msg.get("role") == "system":
                system_prompt = msg.get("content", "")
            else:
                conversation_messages.append(msg)
        
        if not system_prompt:
            system_prompt = self.config.get_system_prompt()
        
        # Combine messages into a single prompt for Titan
        prompt = system_prompt + "\n\n" if system_prompt else ""
        
        for msg in conversation_messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            prompt += f"{role.title()}: {content}\n"
        
        prompt += "Assistant:"
        
        return {
            "inputText": prompt,
            "textGenerationConfig": {
                "maxTokenCount": max_tokens,
                "temperature": temperature,
                "topP": kwargs.get("top_p", self.config.top_p),
                "stopSequences": ["User:", "Human:"]
            }
        }
    
    def _prepare_llama_request(
        self, messages, tools_desc, temperature, max_tokens, **kwargs
    ) -> Dict[str, Any]:
        """Prepare request for Llama models"""
        
        # For Llama, check if system message is already in messages
        has_system_message = any(msg.get("role") == "system" for msg in messages)
        
        formatted_messages = []
        
        if not has_system_message:
            # Add default system message if none present
            formatted_messages.append({
                "role": "system",
                "content": self.config.get_system_prompt()
            })
        
        formatted_messages.extend(messages)
        
        return {
            "messages": formatted_messages,
            "max_gen_len": max_tokens,
            "temperature": temperature,
            "top_p": kwargs.get("top_p", self.config.top_p),
        }
    
    def _prepare_generic_request(
        self, messages, tools_desc, temperature, max_tokens, **kwargs
    ) -> Dict[str, Any]:
        """Prepare generic request format"""
        
        # Extract system prompt from messages for generic format
        system_prompt = None
        conversation_messages = []
        
        for msg in messages:
            if msg.get("role") == "system":
                system_prompt = msg.get("content", "")
            else:
                conversation_messages.append(msg)
        
        if not system_prompt:
            system_prompt = self.config.get_system_prompt()
        
        return {
            "messages": conversation_messages,
            "system_prompt": system_prompt,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "tools": tools_desc,
            **kwargs
        }
    
    async def _make_request_with_retries(
        self, model_id: str, request_body: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Make Bedrock API request with retry logic"""
        
        last_exception = None
        
        for attempt in range(self.config.max_retries + 1):
            try:
                # Convert to async
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(
                    None,
                    lambda: self._client.invoke_model(
                        modelId=model_id,
                        body=json.dumps(request_body),
                        contentType="application/json",
                        accept="application/json"
                    )
                )
                
                # Parse response
                response_body = json.loads(response['body'].read())
                
                # Update request tracking
                self._last_request_time = time.time()
                self._request_count += 1
                
                return response_body
                
            except (BotoCoreError, ClientError) as e:
                last_exception = e
                # Safely extract error code, handling None response
                response = getattr(e, 'response', None)
                error_code = ''
                error_message = str(e)
                if response and isinstance(response, dict):
                    error_code = response.get('Error', {}).get('Code', '')
                
                # Check for context length issues
                if error_code == 'ValidationException' and 'Input is too long' in error_message:
                    # This is a context length issue, enhance the error message
                    enhanced_message = (
                        f"Input is too long for the model's context window. "
                        f"Current conversation strategy: {self.config.conversation_strategy}, "
                        f"max messages: {self.config.max_conversation_messages}. "
                        f"Consider reducing max_conversation_messages or changing conversation_strategy. "
                        f"Original error: {error_message}"
                    )
                    last_exception = BedrockClientError(enhanced_message)
                
                # Don't retry on certain errors
                if error_code in ['ValidationException', 'AccessDeniedException']:
                    break
                
                # Don't retry on last attempt
                if attempt == self.config.max_retries:
                    break
                
                # Calculate delay
                delay = self._calculate_retry_delay(attempt)
                logger.warning(f"Request failed (attempt {attempt + 1}), retrying in {delay}s: {str(e)}")
                
                await asyncio.sleep(delay)
            
            except Exception as e:
                last_exception = e
                
                # Check if it's a timeout error that we can retry
                is_timeout = (
                    'ReadTimeoutError' in str(type(e)) or
                    'timeout' in str(e).lower() or
                    'timed out' in str(e).lower()
                )
                
                # Retry timeout errors, but not on last attempt
                if is_timeout and attempt < self.config.max_retries:
                    delay = self._calculate_retry_delay(attempt)
                    logger.warning(f"Timeout error (attempt {attempt + 1}), retrying in {delay}s: {str(e)}")
                    await asyncio.sleep(delay)
                    continue
                
                # Don't retry other unexpected errors
                break
        
        raise BedrockClientError(f"Request failed after {self.config.max_retries + 1} attempts: {str(last_exception)}")
    
    def _calculate_retry_delay(self, attempt: int) -> float:
        """Calculate delay for retry with exponential backoff"""
        
        base_delay = self.config.retry_delay
        
        if self.config.exponential_backoff:
            delay = base_delay * (2 ** attempt)
        else:
            delay = base_delay
        
        # Add jitter to prevent thundering herd
        import random
        jitter = random.uniform(0.1, 0.3) * delay
        
        return min(delay + jitter, 60.0)  # Cap at 60 seconds
    
    def _parse_response(self, response: Dict[str, Any], model_id: str) -> Dict[str, Any]:
        """Parse and format model response"""
        
        # Check if response is None or invalid
        if response is None:
            logger.error("Received None response from Bedrock API")
            return {
                "content": "I received an empty response from the AI service.",
                "tool_calls": [],
                "metadata": {"error": "None response"}
            }
        
        if not isinstance(response, dict):
            logger.error(f"Received invalid response type: {type(response)}")
            return {
                "content": "I received an invalid response format from the AI service.",
                "tool_calls": [],
                "metadata": {"error": f"Invalid response type: {type(response)}"}
            }
        
        try:
            logger.debug(f"Parsing response for model {model_id}: {response}")
            
            if model_id.startswith("anthropic.claude") or model_id.startswith("us.anthropic.claude"):
                return self._parse_claude_response(response)
            elif model_id.startswith("amazon.titan"):
                return self._parse_titan_response(response)
            elif model_id.startswith("meta.llama"):
                return self._parse_llama_response(response)
            elif model_id.startswith("openai.gpt-oss"):
                return self._parse_openai_gpt_response(response)
            else:
                return self._parse_generic_response(response)
                
        except Exception as e:
            logger.exception(f"Failed to parse response: {str(e)}")
            logger.error(f"Response content: {response}")
            return {
                "content": "I encountered an error processing the response.",
                "tool_calls": [],
                "metadata": {"error": str(e)}
            }
    
    def _parse_claude_response(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """Parse Claude model response"""
        
        content = ""
        tool_calls = []
        
        # Extract content from response
        if "content" in response:
            for item in response["content"]:
                if item.get("type") == "text":
                    content += item.get("text", "")
                elif item.get("type") == "tool_use":
                    tool_calls.append({
                        "id": item.get("id"),
                        "name": item.get("name"),
                        "arguments": item.get("input", {})
                    })
        
        return {
            "content": content,
            "tool_calls": tool_calls,
            "metadata": {
                "model": response.get("model", "unknown"),
                "usage": response.get("usage", {}),
                "stop_reason": response.get("stop_reason")
            }
        }
    
    def _parse_openai_gpt_response(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """Parse OpenAI GPT OSS model response"""
        
        choices = response.get("choices", [])
        if not choices:
            return {"content": "", "tool_calls": [], "metadata": {}}
        
        message = choices[0].get("message", {})
        content = message.get("content", "")
        
        # Extract tool calls
        tool_calls = []
        if "tool_calls" in message:
            for tool_call in message["tool_calls"]:
                if tool_call.get("type") == "function":
                    function = tool_call.get("function", {})
                    tool_calls.append({
                        "id": tool_call.get("id"),
                        "name": function.get("name"),
                        "arguments": json.loads(function.get("arguments", "{}"))
                    })
        
        return {
            "content": content,
            "tool_calls": tool_calls,
            "metadata": {
                "usage": response.get("usage", {}),
                "finish_reason": choices[0].get("finish_reason")
            }
        }
    
    def _parse_titan_response(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """Parse Titan model response"""
        
        results = response.get("results", [])
        content = results[0].get("outputText", "") if results else ""
        
        return {
            "content": content,
            "tool_calls": [],  # Titan doesn't support tool calling
            "metadata": {
                "inputTextTokenCount": response.get("inputTextTokenCount"),
                "outputTextTokenCount": results[0].get("tokenCount") if results else 0
            }
        }
    
    def _parse_llama_response(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """Parse Llama model response"""
        
        content = response.get("generation", "")
        
        return {
            "content": content,
            "tool_calls": [],
            "metadata": {
                "generation_token_count": response.get("generation_token_count"),
                "prompt_token_count": response.get("prompt_token_count"),
                "stop_reason": response.get("stop_reason")
            }
        }
    
    def _parse_generic_response(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """Parse generic model response"""
        
        return {
            "content": response.get("content", response.get("text", "")),
            "tool_calls": response.get("tool_calls", []),
            "metadata": response.get("metadata", {})
        }
    
    def _manage_conversation_history(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Manage conversation history to prevent context length issues
        
        Args:
            messages: Original conversation messages
            
        Returns:
            Trimmed messages that fit within context limits
        """
        if len(messages) <= self.config.max_conversation_messages:
            return messages
        
        logger.info(f"Conversation history has {len(messages)} messages, trimming to {self.config.max_conversation_messages} using {self.config.conversation_strategy} strategy")
        
        if self.config.conversation_strategy == "truncate":
            return self._truncate_messages(messages)
        elif self.config.conversation_strategy == "sliding_window":
            return self._sliding_window_messages(messages)
        elif self.config.conversation_strategy == "smart_prune":
            return self._smart_prune_messages(messages)
        else:
            # Default to sliding window
            return self._sliding_window_messages(messages)
    
    def _truncate_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Simple truncation - keep the most recent messages"""
        if self.config.preserve_system_message and messages and messages[0].get("role") == "system":
            # Keep system message + most recent messages
            system_msg = [messages[0]]
            recent_messages = messages[-(self.config.max_conversation_messages - 1):]
            return system_msg + recent_messages
        else:
            return messages[-self.config.max_conversation_messages:]
    
    def _sliding_window_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Sliding window - preserve system message and recent context"""
        result = []
        
        # Always preserve system message if present and configured
        if self.config.preserve_system_message and messages and messages[0].get("role") == "system":
            result.append(messages[0])
            remaining_messages = messages[1:]
            max_remaining = self.config.max_conversation_messages - 1
        else:
            remaining_messages = messages
            max_remaining = self.config.max_conversation_messages
        
        # Keep the most recent messages
        if len(remaining_messages) > max_remaining:
            result.extend(remaining_messages[-max_remaining:])
        else:
            result.extend(remaining_messages)
        
        return result
    
    def _smart_prune_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Smart pruning - remove tool messages first, then older messages"""
        result = []
        
        # Always preserve system message if present and configured
        if self.config.preserve_system_message and messages and messages[0].get("role") == "system":
            result.append(messages[0])
            remaining_messages = messages[1:]
            max_remaining = self.config.max_conversation_messages - 1
        else:
            remaining_messages = messages
            max_remaining = self.config.max_conversation_messages
        
        if len(remaining_messages) <= max_remaining:
            result.extend(remaining_messages)
            return result
        
        # First pass: filter out tool messages if we have too many
        non_tool_messages = []
        for msg in remaining_messages:
            role = msg.get("role", "")
            if role not in ["tool", "function"] and "tool_call" not in msg:
                non_tool_messages.append(msg)
        
        # If removing tool messages is enough, use that
        if len(non_tool_messages) <= max_remaining:
            result.extend(non_tool_messages)
            return result
        
        # Otherwise, take the most recent non-tool messages
        result.extend(non_tool_messages[-max_remaining:])
        return result
    
    def _check_and_chunk_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Check for large messages and chunk them if necessary
        
        Args:
            messages: List of conversation messages
            
        Returns:
            List of messages with large messages chunked if needed
        """
        if not self.config.enable_message_chunking:
            return messages
        
        result = []
        
        for msg in messages:
            content = msg.get("content", "")
            if isinstance(content, str) and len(content) > self.config.max_message_size:
                logger.info(f"Message size ({len(content)} chars) exceeds max_message_size ({self.config.max_message_size}), chunking...")
                chunked_messages = self._chunk_large_message(msg)
                result.extend(chunked_messages)
            else:
                result.append(msg)
        
        return result
    
    def _chunk_large_message(self, message: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Split a large message into smaller chunks
        
        Args:
            message: The message to chunk
            
        Returns:
            List of chunked messages with proper metadata
        """
        content = message.get("content", "")
        if not isinstance(content, str):
            return [message]  # Cannot chunk non-string content
        
        role = message.get("role", "user")
        
        # Choose chunking strategy
        if self.config.chunking_strategy == "simple":
            chunks = self._simple_chunk(content)
        elif self.config.chunking_strategy == "preserve_context":
            chunks = self._context_aware_chunk(content)
        elif self.config.chunking_strategy == "semantic":
            chunks = self._semantic_chunk(content)
        else:
            chunks = self._context_aware_chunk(content)  # Default fallback
        
        # Create chunked messages with metadata
        chunked_messages = []
        total_chunks = len(chunks)
        
        for i, chunk in enumerate(chunks):
            chunk_number = i + 1
            
            # Create chunk metadata
            chunk_metadata = {
                "is_chunk": True,
                "chunk_number": chunk_number,
                "total_chunks": total_chunks,
                "original_size": len(content),
                "chunk_size": len(chunk)
            }
            
            # Add chunk context information to the content
            if total_chunks > 1:
                chunk_prefix = f"[CHUNK {chunk_number}/{total_chunks}] "
                if chunk_number == 1:
                    chunk_prefix += "This message was too large and has been split into chunks. "
                chunk_content = chunk_prefix + chunk
            else:
                chunk_content = chunk
            
            # Create new message with chunk
            chunked_msg = message.copy()
            chunked_msg["content"] = chunk_content
            
            # Merge metadata properly (chunk metadata takes precedence)
            original_metadata = chunked_msg.get("metadata", {})
            chunked_msg["metadata"] = {**original_metadata, **chunk_metadata}
            
            chunked_messages.append(chunked_msg)
        
        return chunked_messages
    
    def _simple_chunk(self, content: str) -> List[str]:
        """Simple character-based chunking"""
        chunks = []
        chunk_size = self.config.chunk_size
        overlap = self.config.chunk_overlap
        
        i = 0
        while i < len(content):
            # Determine chunk end position
            chunk_end = min(i + chunk_size, len(content))
            chunk = content[i:chunk_end]
            chunks.append(chunk)
            
            # Move to next chunk with overlap
            if chunk_end >= len(content):
                break
            i = chunk_end - overlap
            
        return chunks
    
    def _context_aware_chunk(self, content: str) -> List[str]:
        """Context-aware chunking that tries to break on natural boundaries"""
        chunks = []
        chunk_size = self.config.chunk_size
        overlap = self.config.chunk_overlap
        
        # Natural break points in order of preference
        break_patterns = ['\n\n', '\n', '. ', ', ', ' ']
        
        i = 0
        while i < len(content):
            # Find the ideal chunk end
            ideal_end = min(i + chunk_size, len(content))
            
            if ideal_end >= len(content):
                # Last chunk, take everything remaining
                chunks.append(content[i:])
                break
            
            # Look for a good break point before the ideal end
            best_break = ideal_end
            for pattern in break_patterns:
                # Search backwards from ideal end for pattern
                search_start = max(i + chunk_size // 2, ideal_end - chunk_size // 4)
                last_occurrence = content.rfind(pattern, search_start, ideal_end)
                if last_occurrence > i:
                    best_break = last_occurrence + len(pattern)
                    break
            
            # Extract chunk
            chunk = content[i:best_break].strip()
            if chunk:  # Only add non-empty chunks
                chunks.append(chunk)
            
            # Move to next position with overlap
            i = max(best_break - overlap, i + 1)  # Ensure progress
        
        return chunks
    
    def _semantic_chunk(self, content: str) -> List[str]:
        """
        Semantic chunking that tries to preserve logical units
        Currently falls back to context-aware chunking, but could be enhanced
        with NLP libraries for more intelligent splitting
        """
        # For now, use context-aware chunking
        # In the future, this could use libraries like spacy or nltk
        # to split on sentence or paragraph boundaries more intelligently
        return self._context_aware_chunk(content)
    
    async def _execute_tool_calls(
        self, tool_calls: List[Dict[str, Any]], tools_desc: Optional[Dict]
    ) -> List[Dict[str, Any]]:
        """Execute tool calls (API endpoints)"""
        
        # This will be implemented by the WebSocket handler
        # that has access to the FastAPI app and can make HTTP calls
        # For now, return placeholder
        
        results = []
        for tool_call in tool_calls:
            results.append({
                "tool_call_id": tool_call.get("id"),
                "name": tool_call.get("name"),
                "result": "Tool execution will be handled by WebSocket handler"
            })
        
        return results
    
    def _create_error_response(self, error_message: str) -> Dict[str, Any]:
        """Create error response for graceful degradation"""
        
        return {
            "content": f"I'm experiencing technical difficulties: {error_message}. Please try again in a moment.",
            "tool_calls": [],
            "metadata": {"error": True, "error_message": error_message}
        }
    
    async def _handle_rate_limiting(self):
        """Simple rate limiting to avoid overwhelming the API"""
        
        current_time = time.time()
        time_since_last_request = current_time - self._last_request_time
        
        # Ensure minimum time between requests (basic rate limiting)
        min_interval = 0.1  # 100ms minimum between requests
        if time_since_last_request < min_interval:
            await asyncio.sleep(min_interval - time_since_last_request)
    
    async def health_check(self) -> Dict[str, Any]:
        """Check Bedrock service health"""
        
        try:
            # Simple test request
            test_messages = [{"role": "user", "content": "Hello"}]
            
            response = await self.chat_completion(
                messages=test_messages,
                max_tokens=10,
                temperature=0.1
            )
            
            return {
                "status": "healthy",
                "model": self.config.model_id,
                "region": self.config.aws_region,
                "response_received": bool(response.get("content"))
            }
            
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "model": self.config.model_id,
                "region": self.config.aws_region
            }